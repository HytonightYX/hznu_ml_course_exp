{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 使用 OLSLinerRegression/GDLinearRegression 预测红酒口感"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 7.4  ,  0.7  ,  0.   , ...,  3.51 ,  0.56 ,  9.4  ],\n       [ 7.8  ,  0.88 ,  0.   , ...,  3.2  ,  0.68 ,  9.8  ],\n       [ 7.8  ,  0.76 ,  0.04 , ...,  3.26 ,  0.65 ,  9.8  ],\n       ...,\n       [ 6.3  ,  0.51 ,  0.13 , ...,  3.42 ,  0.75 , 11.   ],\n       [ 5.9  ,  0.645,  0.12 , ...,  3.57 ,  0.71 , 10.2  ],\n       [ 6.   ,  0.31 ,  0.47 , ...,  3.39 ,  0.66 , 11.   ]])"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data = np.genfromtxt('dataset/winequality-red.csv', delimiter=';', skip_header=True)\n",
    "X = data[:, :-1]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([5., 5., 5., ..., 6., 5., 6.])"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "y = data[:, -1]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 OLSLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_regression import OLSLinearRegression\n",
    "# 创建模型\n",
    "ols_lr = OLSLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "ols_lr.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[5.19949366 5.19200469 5.37299894 5.97961538 5.70461562 5.81195376\n 5.91361891 5.02260622 5.4070703  6.21315142 5.88643704 6.65692252\n 5.80143157 5.68691102 6.16026974 6.45380337 5.92857293 5.28945339\n 5.44424214 5.30478465 6.4061314  5.62666303 5.26706272 5.12015464\n 6.01646271 5.38798096 5.68696837 5.05696541 5.93744985 5.23902573\n 4.92562889 5.52809647 6.09900935 6.38338581 5.69758526 5.17890552\n 5.56650569 6.20181361 5.79605017 5.57790364 5.19793443 5.84428259\n 5.90756364 5.28615708 5.30922507 6.21046369 5.35427708 5.31809233\n 5.60073092 5.44169969 5.60789975 6.23845527 6.20556585 5.41547264\n 5.5228931  5.58753605 5.64351098 6.42287043 4.80708182 4.91860034\n 5.25816384 5.82681623 5.31147294 5.23675715 5.27146928 6.42073908\n 5.36541979 5.19942465 6.06268461 5.59421755 6.08814459 5.32108176\n 5.1563941  5.75002904 4.95691948 5.63671849 5.0292048  5.36568679\n 5.47343205 5.58528808 5.54246846 6.4327241  5.2310819  5.80203852\n 6.53305061 5.40369685 5.72204852 4.8746533  5.36445471 5.51453515\n 5.11687074 5.06791033 5.45446955 6.47092873 5.07285633 5.24403218\n 5.26227394 5.9464724  5.2542873  5.82466164 6.00520364 5.65700461\n 5.47173352 5.9002557  5.82915658 6.39885045 5.47781756 5.41292952\n 6.12184123 6.08284506 5.19793443 4.77024852 5.33548119 6.28139031\n 5.12142903 5.21185332 5.7645635  5.71660188 5.27189125 6.24051255\n 6.13206588 6.62509373 5.3440949  4.5197973  6.28550256 5.71968547\n 5.95580238 6.27624434 5.53385276 5.96960002 6.56156546 5.27193694\n 5.7647128  4.99198096 5.87405743 4.44264644 5.34247589 5.18562568\n 5.15621265 5.9330915  5.14804024 5.63619586 5.78396803 5.81344724\n 6.3931404  6.36888285 5.81274132 5.19238978 5.09236847 6.27151236\n 5.00894486 6.33397061 5.31951722 5.85147965 6.31526866 5.6996104\n 5.08563271 4.88484061 5.20805325 6.13101027 4.96311505 5.97934539\n 5.74519797 5.50321192 5.92616983 5.03802732 5.09649474 5.85240938\n 5.9583657  6.35452016 5.39536636 5.56031122 5.4074717  5.36788006\n 5.25011581 5.0350569  5.56090921 5.41547264 5.25468741 5.78285154\n 5.24108357 5.51585716 5.30405561 5.24456726 5.54727095 6.17640076\n 5.74089375 5.27633515 5.71009148 6.09185784 5.63128579 6.0009443\n 5.61843951 6.54097946 5.31629959 5.63242476 5.55306335 5.17489328\n 5.63242476 5.29701698 5.55441102 5.76638687 5.15527494 6.01905602\n 6.19413596 5.61824761 5.97961538 6.10395963 6.57883442 5.79203906\n 5.0350569  5.73979605 5.26573465 5.48437501 5.81365581 5.2975034\n 4.76578908 5.83265496 5.00184796 5.31951722 5.98144888 5.88195564\n 4.82987371 6.06268461 5.24456726 5.55432078 5.00186372 5.87298286\n 5.93436334 5.51216351 6.47782087 6.44049897 5.55695107 5.51635945\n 5.10201202 5.05805611 5.60966682 5.25456002 6.08437292 5.04316072\n 5.54696206 5.46358076 5.74519797 5.71660188 5.76476378 4.97528828\n 5.03031734 5.0252385  5.3800696  5.90981437 5.51372707 5.26292087\n 5.80795804 5.86428992 6.68922296 5.85275731 6.49330371 6.1812492\n 6.14708942 5.7432509  6.06948692 6.1058273  5.36232544 6.72598398\n 5.71186024 5.62895134 6.41494612 5.18815653 5.73979605 5.30922507\n 5.28069922 5.50583787 5.65611986 5.79605017 6.39767067 5.53132621\n 6.10419214 5.07483925 5.25203736 5.39196011 5.98906594 5.2018294\n 5.90088858 5.47356265 5.63587176 5.44538681 6.66000179 5.72840825\n 5.5799947  6.04866604 5.2999377  5.41236654 6.1777922  5.30148142\n 6.17283422 5.50259722 5.85230292 5.72282029 5.38093471 5.79417179\n 5.43167177 6.10705924 5.68827816 6.46253135 6.08796621 5.73341958\n 5.28912725 6.42287043 5.84241392 5.90294415 6.5766227  5.06500342\n 5.99778958 5.85002091 6.47474701 5.23846899 5.07158916 5.86134364\n 5.17991461 5.34418429 6.18782244 5.19108418 4.95959111 6.49128722\n 5.07062688 5.10082145 6.18245694 5.19915577 5.04134382 5.31787018\n 6.41709321 5.25843884 6.34084003 5.1256296  6.07196486 5.31809233\n 5.10672097 4.99201402 5.71052021 5.25092863 6.19084278 5.84890906\n 6.41926815 5.84075924 6.59969792 5.07129983 5.32888387 5.3517619\n 6.17884803 5.10880399 6.48646342 6.21046369 5.55384164 5.80795804\n 5.53214936 6.09007033 6.13367259 5.21853433 4.97661038 5.4014584\n 6.05842166 5.02058693 6.22769977 5.90756364 6.10214925 5.17442379\n 5.08110542 5.88088951 5.33293737 5.19008161 5.06832212 5.32524615\n 6.55838694 5.59474969 4.92577233 6.2990408  4.86140608 6.32311213\n 6.19361373 5.36656669 6.13426429 5.34226233 5.62091039 5.66113787\n 5.3965163  6.20181361 5.55320182 5.31167989 5.65937293 6.35795617\n 5.87262265 4.92124307 5.90844439 6.12825124 5.27189125 5.16242227\n 4.99336495 6.08610023 5.73135996 5.07401596 6.47699824 5.87083905\n 4.96262519 6.48714117 6.26513566 6.4573267  6.09900935 5.46064128\n 5.6115248  5.16192786 5.27193694 6.34994799 5.91495794 5.49525967\n 5.15125602 5.69679595 5.18219055 6.05293908 5.42383477 6.10419214\n 5.14181576 6.17884803 5.3909196  6.13781469 5.55221721 6.10633024\n 5.74039336 5.61315316 5.87799681 5.8474649  5.27292604 5.77767909\n 5.66113787 5.56452713 6.93694128 5.16946607 6.28039677 5.18851112\n 5.53545863 5.44848975 5.34594326 5.45499844 5.08633691 4.94856096\n 6.02878414 5.2771504  5.29271438 5.22123967 6.13426429 5.37992116\n 5.04805757 5.04105923 6.36993456 5.76404452 5.15911041 4.93073863\n 4.6753453  6.05038658 6.72611217 5.95697082 5.71009148 5.95509814\n 5.85002091 6.04777007 6.42239992 5.71315153 5.83521989 5.59597143\n 6.4121773  5.32327252 5.74496044 5.09270848 5.56983186 5.69407252\n 4.71573532 6.42294302 5.19722549 5.71315153 6.04162393 5.57205903]\n"
    }
   ],
   "source": [
    "y_pred = ols_lr.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "在测试集上的MSE: 0.4345\n"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 以均方误差(MSE)衡量回归模型的性能\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('在测试集上的MSE: {:.4f}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "在训练集上的MSE: 0.4114\n"
    }
   ],
   "source": [
    "y_train_pred = ols_lr.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "print('在训练集上的MSE: {:.4f}'.format(mse_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型在训练集与测试集性能相差不大,说明未发生过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "在测试集上的MAE: 0.4966\n"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('在测试集上的MAE: {:.4f}'.format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 GDLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "from linear_regression import GDLinearRegression\n",
    "gd_lr = GDLinearRegression(n_iter=3000, eta=0.001, tol=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 Loss: 9.412393881658998\n   1 Loss: 14.932911320314846\n"
    }
   ],
   "source": [
    "# 这样会产生问题\n",
    "gd_lr.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上输出表明,Loss 不降反升,算法就停止了,这说明步长太长,已经\"迈到对面山坡\"上了.这时需调小学习率 eta = 0.0001 再试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " 0.5301720344661321\n2331 Loss: 0.5301511728514866\n2332 Loss: 0.5301303205846931\n2333 Loss: 0.5301094776613979\n2334 Loss: 0.5300886440772495\n2335 Loss: 0.5300678198278984\n2336 Loss: 0.5300470049089975\n2337 Loss: 0.5300261993162008\n2338 Loss: 0.5300054030451654\n2339 Loss: 0.52998461609155\n2340 Loss: 0.5299638384510155\n2341 Loss: 0.5299430701192247\n2342 Loss: 0.529922311091843\n2343 Loss: 0.5299015613645371\n2344 Loss: 0.5298808209329762\n2345 Loss: 0.5298600897928317\n2346 Loss: 0.5298393679397767\n2347 Loss: 0.5298186553694868\n2348 Loss: 0.529797952077639\n2349 Loss: 0.5297772580599133\n2350 Loss: 0.5297565733119909\n2351 Loss: 0.5297358978295553\n2352 Loss: 0.5297152316082924\n2353 Loss: 0.52969457464389\n2354 Loss: 0.5296739269320376\n2355 Loss: 0.5296532884684273\n2356 Loss: 0.5296326592487527\n2357 Loss: 0.5296120392687099\n2358 Loss: 0.5295914285239971\n2359 Loss: 0.5295708270103139\n2360 Loss: 0.5295502347233626\n2361 Loss: 0.5295296516588475\n2362 Loss: 0.5295090778124746\n2363 Loss: 0.5294885131799522\n2364 Loss: 0.5294679577569905\n2365 Loss: 0.5294474115393021\n2366 Loss: 0.529426874522601\n2367 Loss: 0.5294063467026037\n2368 Loss: 0.5293858280750288\n2369 Loss: 0.5293653186355968\n2370 Loss: 0.52934481838003\n2371 Loss: 0.5293243273040532\n2372 Loss: 0.529303845403393\n2373 Loss: 0.5292833726737779\n2374 Loss: 0.5292629091109387\n2375 Loss: 0.529242454710608\n2376 Loss: 0.5292220094685205\n2377 Loss: 0.5292015733804131\n2378 Loss: 0.5291811464420244\n2379 Loss: 0.5291607286490955\n2380 Loss: 0.529140319997369\n2381 Loss: 0.5291199204825899\n2382 Loss: 0.529099530100505\n2383 Loss: 0.5290791488468635\n2384 Loss: 0.529058776717416\n2385 Loss: 0.5290384137079155\n2386 Loss: 0.5290180598141173\n2387 Loss: 0.5289977150317781\n2388 Loss: 0.528977379356657\n2389 Loss: 0.5289570527845151\n2390 Loss: 0.5289367353111153\n2391 Loss: 0.528916426932223\n2392 Loss: 0.5288961276436048\n2393 Loss: 0.5288758374410303\n2394 Loss: 0.5288555563202703\n2395 Loss: 0.5288352842770979\n2396 Loss: 0.5288150213072885\n2397 Loss: 0.5287947674066188\n2398 Loss: 0.5287745225708684\n2399 Loss: 0.5287542867958182\n2400 Loss: 0.5287340600772513\n2401 Loss: 0.5287138424109529\n2402 Loss: 0.5286936337927102\n2403 Loss: 0.5286734342183125\n2404 Loss: 0.5286532436835507\n2405 Loss: 0.528633062184218\n2406 Loss: 0.5286128897161098\n2407 Loss: 0.5285927262750231\n2408 Loss: 0.5285725718567568\n2409 Loss: 0.5285524264571123\n2410 Loss: 0.5285322900718927\n2411 Loss: 0.5285121626969033\n2412 Loss: 0.5284920443279509\n2413 Loss: 0.5284719349608449\n2414 Loss: 0.5284518345913961\n2415 Loss: 0.5284317432154177\n2416 Loss: 0.5284116608287251\n2417 Loss: 0.5283915874271348\n2418 Loss: 0.5283715230064663\n2419 Loss: 0.5283514675625403\n2420 Loss: 0.52833142109118\n2421 Loss: 0.5283113835882104\n2422 Loss: 0.5282913550494586\n2423 Loss: 0.5282713354707532\n2424 Loss: 0.5282513248479254\n2425 Loss: 0.5282313231768081\n2426 Loss: 0.528211330453236\n2427 Loss: 0.528191346673046\n2428 Loss: 0.5281713718320774\n2429 Loss: 0.5281514059261703\n2430 Loss: 0.528131448951168\n2431 Loss: 0.528111500902915\n2432 Loss: 0.5280915617772582\n2433 Loss: 0.5280716315700462\n2434 Loss: 0.5280517102771297\n2435 Loss: 0.5280317978943613\n2436 Loss: 0.5280118944175954\n2437 Loss: 0.5279919998426891\n2438 Loss: 0.5279721141655004\n2439 Loss: 0.5279522373818899\n2440 Loss: 0.5279323694877203\n2441 Loss: 0.5279125104788558\n2442 Loss: 0.5278926603511627\n2443 Loss: 0.5278728191005094\n2444 Loss: 0.5278529867227665\n2445 Loss: 0.5278331632138057\n2446 Loss: 0.5278133485695015\n2447 Loss: 0.5277935427857301\n2448 Loss: 0.5277737458583696\n2449 Loss: 0.5277539577832999\n2450 Loss: 0.527734178556403\n2451 Loss: 0.5277144081735632\n2452 Loss: 0.527694646630666\n2453 Loss: 0.5276748939235996\n2454 Loss: 0.5276551500482537\n2455 Loss: 0.5276354150005199\n2456 Loss: 0.527615688776292\n2457 Loss: 0.5275959713714656\n2458 Loss: 0.5275762627819385\n2459 Loss: 0.5275565630036101\n2460 Loss: 0.5275368720323818\n2461 Loss: 0.5275171898641571\n2462 Loss: 0.5274975164948411\n2463 Loss: 0.5274778519203416\n2464 Loss: 0.5274581961365674\n2465 Loss: 0.5274385491394299\n2466 Loss: 0.5274189109248422\n2467 Loss: 0.5273992814887191\n2468 Loss: 0.5273796608269777\n2469 Loss: 0.527360048935537\n2470 Loss: 0.5273404458103177\n2471 Loss: 0.5273208514472427\n2472 Loss: 0.5273012658422366\n2473 Loss: 0.5272816889912261\n2474 Loss: 0.5272621208901395\n2475 Loss: 0.5272425615349076\n2476 Loss: 0.5272230109214626\n2477 Loss: 0.5272034690457389\n2478 Loss: 0.5271839359036727\n2479 Loss: 0.5271644114912024\n2480 Loss: 0.5271448958042677\n2481 Loss: 0.5271253888388108\n2482 Loss: 0.5271058905907758\n2483 Loss: 0.5270864010561084\n2484 Loss: 0.5270669202307562\n2485 Loss: 0.5270474481106692\n2486 Loss: 0.5270279846917989\n2487 Loss: 0.5270085299700986\n2488 Loss: 0.5269890839415241\n2489 Loss: 0.5269696466020324\n2490 Loss: 0.526950217947583\n2491 Loss: 0.5269307979741369\n2492 Loss: 0.5269113866776571\n2493 Loss: 0.5268919840541092\n2494 Loss: 0.5268725900994592\n2495 Loss: 0.5268532048096766\n2496 Loss: 0.5268338281807314\n2497 Loss: 0.526814460208597\n2498 Loss: 0.5267951008892474\n2499 Loss: 0.5267757502186593\n2500 Loss: 0.5267564081928108\n2501 Loss: 0.5267370748076824\n2502 Loss: 0.5267177500592557\n2503 Loss: 0.5266984339435152\n2504 Loss: 0.5266791264564467\n2505 Loss: 0.5266598275940378\n2506 Loss: 0.5266405373522786\n2507 Loss: 0.5266212557271603\n2508 Loss: 0.5266019827146767\n2509 Loss: 0.5265827183108232\n2510 Loss: 0.5265634625115967\n2511 Loss: 0.5265442153129968\n2512 Loss: 0.5265249767110244\n2513 Loss: 0.5265057467016825\n2514 Loss: 0.5264865252809758\n2515 Loss: 0.5264673124449113\n2516 Loss: 0.5264481081894974\n2517 Loss: 0.5264289125107448\n2518 Loss: 0.5264097254046656\n2519 Loss: 0.5263905468672744\n2520 Loss: 0.526371376894587\n2521 Loss: 0.5263522154826218\n2522 Loss: 0.5263330626273985\n2523 Loss: 0.526313918324939\n2524 Loss: 0.5262947825712669\n2525 Loss: 0.5262756553624078\n2526 Loss: 0.5262565366943893\n2527 Loss: 0.5262374265632405\n2528 Loss: 0.5262183249649925\n2529 Loss: 0.5261992318956784\n2530 Loss: 0.5261801473513336\n2531 Loss: 0.5261610713279942\n2532 Loss: 0.5261420038216994\n2533 Loss: 0.5261229448284895\n2534 Loss: 0.526103894344407\n2535 Loss: 0.5260848523654963\n2536 Loss: 0.5260658188878032\n2537 Loss: 0.5260467939073763\n2538 Loss: 0.5260277774202647\n2539 Loss: 0.526008769422521\n2540 Loss: 0.5259897699101982\n2541 Loss: 0.525970778879352\n2542 Loss: 0.5259517963260398\n2543 Loss: 0.5259328222463208\n2544 Loss: 0.525913856636256\n2545 Loss: 0.5258948994919083\n2546 Loss: 0.5258759508093425\n2547 Loss: 0.5258570105846254\n2548 Loss: 0.5258380788138252\n2549 Loss: 0.5258191554930125\n2550 Loss: 0.5258002406182596\n2551 Loss: 0.5257813341856401\n2552 Loss: 0.5257624361912303\n2553 Loss: 0.5257435466311079\n2554 Loss: 0.5257246655013524\n2555 Loss: 0.5257057927980456\n2556 Loss: 0.5256869285172704\n2557 Loss: 0.5256680726551123\n2558 Loss: 0.5256492252076579\n2559 Loss: 0.5256303861709966\n2560 Loss: 0.5256115555412187\n2561 Loss: 0.525592733314417\n2562 Loss: 0.5255739194866857\n2563 Loss: 0.5255551140541211\n2564 Loss: 0.5255363170128213\n2565 Loss: 0.5255175283588863\n2566 Loss: 0.5254987480884178\n2567 Loss: 0.5254799761975194\n2568 Loss: 0.5254612126822964\n2569 Loss: 0.5254424575388562\n2570 Loss: 0.5254237107633082\n2571 Loss: 0.5254049723517629\n2572 Loss: 0.5253862423003333\n2573 Loss: 0.5253675206051338\n2574 Loss: 0.5253488072622812\n2575 Loss: 0.5253301022678937\n2576 Loss: 0.5253114056180911\n2577 Loss: 0.5252927173089957\n2578 Loss: 0.5252740373367312\n2579 Loss: 0.5252553656974231\n2580 Loss: 0.5252367023871989\n2581 Loss: 0.5252180474021878\n2582 Loss: 0.525199400738521\n2583 Loss: 0.5251807623923312\n2584 Loss: 0.5251621323597534\n2585 Loss: 0.525143510636924\n2586 Loss: 0.5251248972199815\n2587 Loss: 0.5251062921050658\n2588 Loss: 0.5250876952883192\n2589 Loss: 0.5250691067658856\n2590 Loss: 0.5250505265339104\n2591 Loss: 0.5250319545885412\n2592 Loss: 0.5250133909259272\n2593 Loss: 0.5249948355422197\n2594 Loss: 0.5249762884335717\n2595 Loss: 0.5249577495961374\n2596 Loss: 0.5249392190260739\n2597 Loss: 0.5249206967195392\n2598 Loss: 0.524902182672694\n2599 Loss: 0.5248836768816996\n2600 Loss: 0.5248651793427203\n2601 Loss: 0.5248466900519214\n2602 Loss: 0.5248282090054706\n2603 Loss: 0.5248097361995369\n2604 Loss: 0.5247912716302913\n2605 Loss: 0.5247728152939068\n2606 Loss: 0.5247543671865579\n2607 Loss: 0.5247359273044211\n2608 Loss: 0.5247174956436748\n2609 Loss: 0.5246990722004987\n2610 Loss: 0.5246806569710749\n2611 Loss: 0.5246622499515868\n2612 Loss: 0.5246438511382201\n2613 Loss: 0.524625460527162\n2614 Loss: 0.5246070781146015\n2615 Loss: 0.5245887038967294\n2616 Loss: 0.5245703378697384\n2617 Loss: 0.5245519800298227\n2618 Loss: 0.5245336303731789\n2619 Loss: 0.5245152888960047\n2620 Loss: 0.5244969555945\n2621 Loss: 0.5244786304648665\n2622 Loss: 0.5244603135033076\n2623 Loss: 0.5244420047060281\n2624 Loss: 0.5244237040692354\n2625 Loss: 0.5244054115891381\n2626 Loss: 0.5243871272619469\n2627 Loss: 0.5243688510838738\n2628 Loss: 0.524350583051133\n2629 Loss: 0.5243323231599406\n2630 Loss: 0.5243140714065141\n2631 Loss: 0.524295827787073\n2632 Loss: 0.5242775922978384\n2633 Loss: 0.5242593649350337\n2634 Loss: 0.5242411456948836\n2635 Loss: 0.5242229345736141\n2636 Loss: 0.5242047315674544\n2637 Loss: 0.5241865366726343\n2638 Loss: 0.5241683498853855\n2639 Loss: 0.5241501712019421\n2640 Loss: 0.524132000618539\n2641 Loss: 0.524113838131414\n2642 Loss: 0.5240956837368058\n2643 Loss: 0.5240775374309553\n2644 Loss: 0.524059399210105\n2645 Loss: 0.5240412690704994\n2646 Loss: 0.5240231470083844\n2647 Loss: 0.5240050330200078\n2648 Loss: 0.5239869271016194\n2649 Loss: 0.5239688292494706\n2650 Loss: 0.5239507394598146\n2651 Loss: 0.5239326577289061\n2652 Loss: 0.5239145840530021\n2653 Loss: 0.5238965184283609\n2654 Loss: 0.5238784608512427\n2655 Loss: 0.5238604113179095\n2656 Loss: 0.5238423698246251\n2657 Loss: 0.523824336367655\n2658 Loss: 0.5238063109432664\n2659 Loss: 0.5237882935477283\n2660 Loss: 0.5237702841773116\n2661 Loss: 0.5237522828282889\n2662 Loss: 0.5237342894969342\n2663 Loss: 0.5237163041795238\n2664 Loss: 0.5236983268723353\n2665 Loss: 0.5236803575716487\n2666 Loss: 0.5236623962737448\n2667 Loss: 0.5236444429749069\n2668 Loss: 0.5236264976714199\n2669 Loss: 0.5236085603595702\n2670 Loss: 0.523590631035646\n2671 Loss: 0.5235727096959377\n2672 Loss: 0.5235547963367368\n2673 Loss: 0.5235368909543372\n2674 Loss: 0.5235189935450337\n2675 Loss: 0.5235011041051238\n2676 Loss: 0.5234832226309061\n2677 Loss: 0.5234653491186811\n2678 Loss: 0.5234474835647513\n2679 Loss: 0.5234296259654204\n2680 Loss: 0.5234117763169944\n2681 Loss: 0.5233939346157807\n2682 Loss: 0.5233761008580885\n2683 Loss: 0.523358275040229\n2684 Loss: 0.5233404571585144\n2685 Loss: 0.5233226472092597\n2686 Loss: 0.5233048451887808\n2687 Loss: 0.5232870510933958\n2688 Loss: 0.5232692649194243\n2689 Loss: 0.5232514866631874\n2690 Loss: 0.5232337163210087\n2691 Loss: 0.5232159538892126\n2692 Loss: 0.523198199364126\n2693 Loss: 0.5231804527420771\n2694 Loss: 0.523162714019396\n2695 Loss: 0.5231449831924143\n2696 Loss: 0.5231272602574658\n2697 Loss: 0.5231095452108854\n2698 Loss: 0.5230918380490103\n2699 Loss: 0.5230741387681789\n2700 Loss: 0.5230564473647319\n2701 Loss: 0.5230387638350114\n2702 Loss: 0.523021088175361\n2703 Loss: 0.5230034203821264\n2704 Loss: 0.5229857604516548\n2705 Loss: 0.5229681083802955\n2706 Loss: 0.5229504641643989\n2707 Loss: 0.5229328278003177\n2708 Loss: 0.5229151992844059\n2709 Loss: 0.5228975786130193\n2710 Loss: 0.5228799657825157\n2711 Loss: 0.5228623607892545\n2712 Loss: 0.5228447636295962\n2713 Loss: 0.5228271742999041\n2714 Loss: 0.5228095927965424\n2715 Loss: 0.5227920191158772\n2716 Loss: 0.5227744532542766\n2717 Loss: 0.5227568952081101\n2718 Loss: 0.522739344973749\n2719 Loss: 0.522721802547566\n2720 Loss: 0.5227042679259362\n2721 Loss: 0.5226867411052358\n2722 Loss: 0.5226692220818431\n2723 Loss: 0.5226517108521378\n2724 Loss: 0.5226342074125012\n2725 Loss: 0.5226167117593169\n2726 Loss: 0.5225992238889701\n2727 Loss: 0.5225817437978467\n2728 Loss: 0.5225642714823354\n2729 Loss: 0.5225468069388264\n2730 Loss: 0.5225293501637112\n2731 Loss: 0.5225119011533833\n2732 Loss: 0.5224944599042379\n2733 Loss: 0.522477026412672\n2734 Loss: 0.5224596006750836\n2735 Loss: 0.5224421826878735\n2736 Loss: 0.5224247724474431\n2737 Loss: 0.5224073699501964\n2738 Loss: 0.5223899751925387\n2739 Loss: 0.5223725881708768\n2740 Loss: 0.5223552088816196\n2741 Loss: 0.5223378373211772\n2742 Loss: 0.5223204734859618\n2743 Loss: 0.5223031173723873\n2744 Loss: 0.522285768976869\n2745 Loss: 0.5222684282958241\n2746 Loss: 0.5222510953256714\n2747 Loss: 0.5222337700628313\n2748 Loss: 0.5222164525037262\n2749 Loss: 0.5221991426447798\n2750 Loss: 0.5221818404824178\n2751 Loss: 0.5221645460130673\n2752 Loss: 0.5221472592331572\n2753 Loss: 0.5221299801391183\n2754 Loss: 0.5221127087273828\n2755 Loss: 0.5220954449943846\n2756 Loss: 0.5220781889365593\n2757 Loss: 0.5220609405503445\n2758 Loss: 0.5220436998321789\n2759 Loss: 0.5220264667785032\n2760 Loss: 0.5220092413857599\n2761 Loss: 0.5219920236503929\n2762 Loss: 0.521974813568848\n2763 Loss: 0.5219576111375723\n2764 Loss: 0.5219404163530152\n2765 Loss: 0.5219232292116273\n2766 Loss: 0.5219060497098609\n2767 Loss: 0.52188887784417\n2768 Loss: 0.5218717136110106\n2769 Loss: 0.5218545570068396\n2770 Loss: 0.5218374080281166\n2771 Loss: 0.521820266671302\n2772 Loss: 0.5218031329328582\n2773 Loss: 0.5217860068092495\n2774 Loss: 0.5217688882969411\n2775 Loss: 0.521751777392401\n2776 Loss: 0.5217346740920977\n2777 Loss: 0.5217175783925023\n2778 Loss: 0.521700490290087\n2779 Loss: 0.5216834097813259\n2780 Loss: 0.5216663368626946\n2781 Loss: 0.5216492715306704\n2782 Loss: 0.5216322137817325\n2783 Loss: 0.5216151636123613\n2784 Loss: 0.5215981210190394\n2785 Loss: 0.5215810859982505\n2786 Loss: 0.5215640585464802\n2787 Loss: 0.5215470386602162\n2788 Loss: 0.521530026335947\n2789 Loss: 0.5215130215701632\n2790 Loss: 0.5214960243593574\n2791 Loss: 0.5214790347000231\n2792 Loss: 0.5214620525886559\n2793 Loss: 0.5214450780217531\n2794 Loss: 0.5214281109958135\n2795 Loss: 0.5214111515073376\n2796 Loss: 0.5213941995528276\n2797 Loss: 0.521377255128787\n2798 Loss: 0.5213603182317214\n2799 Loss: 0.5213433888581379\n2800 Loss: 0.521326467004545\n2801 Loss: 0.5213095526674534\n2802 Loss: 0.5212926458433746\n2803 Loss: 0.5212757465288227\n2804 Loss: 0.5212588547203127\n2805 Loss: 0.5212419704143617\n2806 Loss: 0.5212250936074879\n2807 Loss: 0.521208224296212\n2808 Loss: 0.5211913624770556\n2809 Loss: 0.5211745081465418\n2810 Loss: 0.521157661301196\n2811 Loss: 0.5211408219375452\n2812 Loss: 0.5211239900521173\n2813 Loss: 0.5211071656414424\n2814 Loss: 0.5210903487020525\n2815 Loss: 0.5210735392304805\n2816 Loss: 0.5210567372232613\n2817 Loss: 0.5210399426769314\n2818 Loss: 0.5210231555880294\n2819 Loss: 0.5210063759530944\n2820 Loss: 0.5209896037686682\n2821 Loss: 0.5209728390312941\n2822 Loss: 0.5209560817375161\n2823 Loss: 0.520939331883881\n2824 Loss: 0.5209225894669364\n2825 Loss: 0.5209058544832322\n2826 Loss: 0.5208891269293193\n2827 Loss: 0.5208724068017504\n2828 Loss: 0.5208556940970803\n2829 Loss: 0.5208389888118645\n2830 Loss: 0.5208222909426611\n2831 Loss: 0.520805600486029\n2832 Loss: 0.5207889174385294\n2833 Loss: 0.5207722417967247\n2834 Loss: 0.5207555735571788\n2835 Loss: 0.5207389127164578\n2836 Loss: 0.5207222592711288\n2837 Loss: 0.5207056132177608\n2838 Loss: 0.5206889745529244\n2839 Loss: 0.520672343273192\n2840 Loss: 0.5206557193751371\n2841 Loss: 0.5206391028553352\n2842 Loss: 0.5206224937103634\n2843 Loss: 0.5206058919368003\n2844 Loss: 0.5205892975312263\n2845 Loss: 0.5205727104902228\n2846 Loss: 0.5205561308103737\n2847 Loss: 0.5205395584882639\n2848 Loss: 0.5205229935204803\n2849 Loss: 0.520506435903611\n2850 Loss: 0.5204898856342457\n2851 Loss: 0.5204733427089762\n2852 Loss: 0.5204568071243953\n2853 Loss: 0.520440278877098\n2854 Loss: 0.5204237579636806\n2855 Loss: 0.5204072443807407\n2856 Loss: 0.520390738124878\n2857 Loss: 0.5203742391926937\n2858 Loss: 0.5203577475807903\n2859 Loss: 0.5203412632857721\n2860 Loss: 0.5203247863042452\n2861 Loss: 0.520308316632817\n2862 Loss: 0.5202918542680963\n2863 Loss: 0.5202753992066943\n2864 Loss: 0.5202589514452229\n2865 Loss: 0.5202425109802961\n2866 Loss: 0.5202260778085293\n2867 Loss: 0.5202096519265397\n2868 Loss: 0.5201932333309456\n2869 Loss: 0.5201768220183677\n2870 Loss: 0.5201604179854274\n2871 Loss: 0.5201440212287485\n2872 Loss: 0.5201276317449556\n2873 Loss: 0.5201112495306756\n2874 Loss: 0.5200948745825366\n2875 Loss: 0.5200785068971683\n2876 Loss: 0.5200621464712021\n2877 Loss: 0.5200457933012708\n2878 Loss: 0.5200294473840091\n2879 Loss: 0.520013108716053\n2880 Loss: 0.5199967772940403\n2881 Loss: 0.5199804531146099\n2882 Loss: 0.5199641361744033\n2883 Loss: 0.5199478264700622\n2884 Loss: 0.5199315239982311\n2885 Loss: 0.5199152287555553\n2886 Loss: 0.519898940738682\n2887 Loss: 0.5198826599442602\n2888 Loss: 0.5198663863689398\n2889 Loss: 0.5198501200093729\n2890 Loss: 0.519833860862213\n2891 Loss: 0.5198176089241151\n2892 Loss: 0.5198013641917356\n2893 Loss: 0.5197851266617329\n2894 Loss: 0.5197688963307668\n2895 Loss: 0.5197526731954984\n2896 Loss: 0.519736457252591\n2897 Loss: 0.5197202484987086\n2898 Loss: 0.5197040469305172\n2899 Loss: 0.5196878525446847\n2900 Loss: 0.5196716653378802\n2901 Loss: 0.5196554853067744\n2902 Loss: 0.5196393124480395\n2903 Loss: 0.5196231467583494\n2904 Loss: 0.5196069882343798\n2905 Loss: 0.5195908368728074\n2906 Loss: 0.5195746926703105\n2907 Loss: 0.5195585556235698\n2908 Loss: 0.5195424257292666\n2909 Loss: 0.5195263029840842\n2910 Loss: 0.5195101873847073\n2911 Loss: 0.5194940789278226\n2912 Loss: 0.5194779776101176\n2913 Loss: 0.519461883428282\n2914 Loss: 0.5194457963790067\n2915 Loss: 0.5194297164589844\n2916 Loss: 0.5194136436649092\n2917 Loss: 0.5193975779934766\n2918 Loss: 0.5193815194413842\n2919 Loss: 0.5193654680053306\n2920 Loss: 0.519349423682016\n2921 Loss: 0.5193333864681425\n2922 Loss: 0.5193173563604135\n2923 Loss: 0.5193013333555342\n2924 Loss: 0.5192853174502107\n2925 Loss: 0.5192693086411514\n2926 Loss: 0.519253306925066\n2927 Loss: 0.5192373122986654\n2928 Loss: 0.5192213247586628\n2929 Loss: 0.5192053443017719\n2930 Loss: 0.5191893709247091\n2931 Loss: 0.5191734046241914\n2932 Loss: 0.5191574453969379\n2933 Loss: 0.5191414932396688\n2934 Loss: 0.5191255481491066\n2935 Loss: 0.5191096101219745\n2936 Loss: 0.5190936791549976\n2937 Loss: 0.5190777552449025\n2938 Loss: 0.5190618383884175\n2939 Loss: 0.519045928582272\n2940 Loss: 0.5190300258231978\n2941 Loss: 0.5190141301079271\n2942 Loss: 0.5189982414331946\n2943 Loss: 0.5189823597957359\n2944 Loss: 0.5189664851922885\n2945 Loss: 0.5189506176195914\n2946 Loss: 0.5189347570743847\n2947 Loss: 0.5189189035534107\n2948 Loss: 0.518903057053413\n2949 Loss: 0.5188872175711363\n2950 Loss: 0.5188713851033275\n2951 Loss: 0.5188555596467346\n2952 Loss: 0.518839741198107\n2953 Loss: 0.5188239297541962\n2954 Loss: 0.5188081253117548\n2955 Loss: 0.5187923278675369\n2956 Loss: 0.5187765374182983\n2957 Loss: 0.5187607539607962\n2958 Loss: 0.5187449774917895\n2959 Loss: 0.5187292080080386\n2960 Loss: 0.518713445506305\n2961 Loss: 0.5186976899833525\n2962 Loss: 0.5186819414359456\n2963 Loss: 0.5186661998608509\n2964 Loss: 0.5186504652548364\n2965 Loss: 0.5186347376146713\n2966 Loss: 0.5186190169371268\n2967 Loss: 0.5186033032189752\n2968 Loss: 0.5185875964569905\n2969 Loss: 0.5185718966479482\n2970 Loss: 0.5185562037886257\n2971 Loss: 0.5185405178758011\n2972 Loss: 0.5185248389062546\n2973 Loss: 0.5185091668767676\n2974 Loss: 0.5184935017841237\n2975 Loss: 0.518477843625107\n2976 Loss: 0.5184621923965037\n2977 Loss: 0.5184465480951016\n2978 Loss: 0.5184309107176898\n2979 Loss: 0.5184152802610585\n2980 Loss: 0.5183996567220004\n2981 Loss: 0.5183840400973091\n2982 Loss: 0.5183684303837796\n2983 Loss: 0.5183528275782086\n2984 Loss: 0.5183372316773941\n2985 Loss: 0.518321642678136\n2986 Loss: 0.5183060605772354\n2987 Loss: 0.518290485371495\n2988 Loss: 0.5182749170577193\n2989 Loss: 0.5182593556327134\n2990 Loss: 0.5182438010932849\n2991 Loss: 0.5182282534362425\n2992 Loss: 0.5182127126583962\n2993 Loss: 0.5181971787565577\n2994 Loss: 0.5181816517275405\n2995 Loss: 0.5181661315681588\n2996 Loss: 0.5181506182752292\n2997 Loss: 0.5181351118455693\n2998 Loss: 0.5181196122759981\n2999 Loss: 0.5181041195633366\n"
    }
   ],
   "source": [
    "# 这样其实还是有问题\n",
    "gd_lr = GDLinearRegression(n_iter=3000, eta=0.0001, tol=0.00001)\n",
    "gd_lr.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这次虽然损失随着迭代下降了,但是迭代到3000次,算法依然没有收敛,最终损失为0.53+,距离之前用最小二乘法算出来的0.417还差很远,并且后面每次迭代的损失下降非常小.这主要是由于 X 中各个特征尺寸小差较大造成的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 8.31963727,  0.52782051,  0.27097561,  2.5388055 ,  0.08746654,\n       15.87492183, 46.46779237,  0.99674668,  3.3111132 ,  0.65814884,\n       10.42298311])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# 观察 X 中各特征的均值\n",
    "X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200626221932131](http://qn-noter.yunxi.site/imagehost/81iwx.png-style1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200626222009284](http://qn-noter.yunxi.site/imagehost/224z6.png-style1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "StandardScaler(copy=True, with_mean=True, with_std=True)"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# 把 X 各特征缩放到相同尺寸,然后重新训练\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 1.16300518, -0.56109349,  1.18960418, -0.15343992, -0.42920622,\n        -0.24552277, -0.81884274, -0.40393429, -1.3585264 ,  1.32787083,\n         0.9143143 ],\n       [-0.08834366, -1.22357159,  0.67611587, -0.23539661, -0.65555137,\n        -0.93017207, -1.09707163, -0.74654696,  0.00621419,  0.15291404,\n         0.72814057],\n       [-0.71401809,  1.70237334, -1.27513967,  1.56765061,  0.20456018,\n        -1.02797912, -1.00432867,  0.13139801,  0.65609066, -0.5273241 ,\n         0.54196685]])"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "X_train_std = ss.transform(X_train)\n",
    "X_test_std = ss.transform(X_test)\n",
    "X_train_std[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.82777707,  0.7638627 , -1.06974435, -0.3173533 , -0.15759205,\n        -0.73455799, -0.38604224,  0.90762984,  2.41075713,  0.33843354,\n        -0.76124922],\n       [-0.20210265, -0.56109349,  0.47072055, -0.15343992, -0.27076462,\n        -0.5389439 ,  0.07767258,  0.45259738,  0.20117713, -1.20756224,\n        -0.94742294],\n       [ 0.30981279, -1.22357159,  1.54904599, -0.39930999,  0.5440779 ,\n         1.02596878,  1.22150247,  0.18493124, -0.31872404, -0.77468343,\n        -0.66816236]])"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "X_test_std[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 Loss: 31.993245084268874\n   1 Loss: 28.89944144724334\n   2 Loss: 26.11002262672163\n   3 Loss: 23.594806091990637\n   4 Loss: 21.326648554724795\n   5 Loss: 19.28113018678798\n   6 Loss: 17.436273814703593\n   7 Loss: 15.772294717022172\n   8 Loss: 14.271377312722926\n   9 Loss: 12.917475565744875\n  10 Loss: 11.696134371002017\n  11 Loss: 10.594329551910782\n  12 Loss: 9.600324404436593\n  13 Loss: 8.703540979998284\n  14 Loss: 7.894444518465321\n  15 Loss: 7.1644396300480295\n  16 Loss: 6.505776986646802\n  17 Loss: 5.9114694235427505\n  18 Loss: 5.375216474634894\n  19 Loss: 4.891336471530726\n  20 Loss: 4.454705430926804\n  21 Loss: 4.060702037714348\n  22 Loss: 3.7051581046344144\n  23 Loss: 3.384313954357132\n  24 Loss: 3.09477822763802\n  25 Loss: 2.833491672616632\n  26 Loss: 2.597694516141412\n  27 Loss: 2.384897058893454\n  28 Loss: 2.192853172614025\n  29 Loss: 2.0195364104139655\n  30 Loss: 1.8631184703911892\n  31 Loss: 1.7219497789852907\n  32 Loss: 1.5945419839891604\n  33 Loss: 1.4795521682100663\n  34 Loss: 1.3757686136864424\n  35 Loss: 1.282097963350957\n  36 Loss: 1.1975536422888617\n  37 Loss: 1.1212454144540327\n  38 Loss: 1.0523699630342263\n  39 Loss: 0.9902023937452437\n  40 Loss: 0.9340885703087242\n  41 Loss: 0.883438200344294\n  42 Loss: 0.8377185979857388\n  43 Loss: 0.796449056804061\n  44 Loss: 0.7591957731693256\n  45 Loss: 0.7255672660813047\n  46 Loss: 0.6952102448116934\n  47 Loss: 0.6678058804868698\n  48 Loss: 0.6430664420526593\n  49 Loss: 0.6207322609486741\n  50 Loss: 0.6005689923221323\n  51 Loss: 0.5823651437678432\n  52 Loss: 0.565929845426686\n  53 Loss: 0.5510908378402396\n  54 Loss: 0.5376926562721495\n  55 Loss: 0.5255949922922373\n  56 Loss: 0.5146712152998677\n  57 Loss: 0.5048070383588674\n  58 Loss: 0.4958993142456353\n  59 Loss: 0.487854948991346\n  60 Loss: 0.4805899214431937\n  61 Loss: 0.4740283984916889\n  62 Loss: 0.46810193662315336\n  63 Loss: 0.4627487613695734\n  64 Loss: 0.45791311705155996\n  65 Loss: 0.4535446799531653\n  66 Loss: 0.4495980287375838\n  67 Loss: 0.4460321665174858\n  68 Loss: 0.4428100895392995\n  69 Loss: 0.43989839793298685\n  70 Loss: 0.43726694442296393\n  71 Loss: 0.43488851729651923\n  72 Loss: 0.4327385542876287\n  73 Loss: 0.43079488436028324\n  74 Loss: 0.4290374946697991\n  75 Loss: 0.42744832024618123\n  76 Loss: 0.42601105418327434\n  77 Loss: 0.4247109763336926\n  78 Loss: 0.4235347987046664\n  79 Loss: 0.42247052592603557\n  80 Loss: 0.42150732932052404\n  81 Loss: 0.42063543324981767\n  82 Loss: 0.4198460125393758\n  83 Loss: 0.41913109990166414\n  84 Loss: 0.4184835023828779\n  85 Loss: 0.4178967259533174\n  86 Loss: 0.41736490744738597\n  87 Loss: 0.41688275313662776\n  88 Loss: 0.4164454832891038\n  89 Loss: 0.4160487821314764\n  90 Loss: 0.4156887526870849\n  91 Loss: 0.4153618760146631\n  92 Loss: 0.41506497441869705\n  93 Loss: 0.41479517824425916\n  94 Loss: 0.4145498959069049\n  95 Loss: 0.414326786842286\n  96 Loss: 0.41412373709088696\n  97 Loss: 0.41393883726103403\n  98 Loss: 0.4137703626383727\n  99 Loss: 0.41361675523260855\n 100 Loss: 0.41347660757270205\n 101 Loss: 0.4133486480801172\n 102 Loss: 0.41323172786633405\n 103 Loss: 0.41312480881582986\n 104 Loss: 0.41302695282926244\n 105 Loss: 0.412937312113803\n 106 Loss: 0.4128551204185829\n 107 Loss: 0.41277968512316904\n 108 Loss: 0.4127103800959554\n 109 Loss: 0.4126466392474621\n 110 Loss: 0.4125879507108422\n 111 Loss: 0.41253385158849804\n 112 Loss: 0.4124839232096607\n 113 Loss: 0.4124377868491637\n 114 Loss: 0.41239509986249123\n 115 Loss: 0.4123555521965591\n 116 Loss: 0.41231886323963796\n 117 Loss: 0.412284778977394\n 118 Loss: 0.4122530694252403\n 119 Loss: 0.41222352631009757\n 120 Loss: 0.41219596097728245\n 121 Loss: 0.4121702025006101\n 122 Loss: 0.4121460959759294\n 123 Loss: 0.4121235009802414\n 124 Loss: 0.4121022901802843\n 125 Loss: 0.4120823480760446\n 126 Loss: 0.41206356986606624\n 127 Loss: 0.4120458604227101\n 128 Loss: 0.4120291333666699\n 129 Loss: 0.41201331023109283\n 130 Loss: 0.4119983197065913\n 131 Loss: 0.41198409695928345\n 132 Loss: 0.4119705830147636\n 133 Loss: 0.4119577242015961\n 134 Loss: 0.41194547164854917\n 135 Loss: 0.41193378083034926\n 136 Loss: 0.4119226111572434\n 137 Loss: 0.41191192560411694\n 138 Loss: 0.41190169037532665\n 139 Loss: 0.4118918746017846\n"
    }
   ],
   "source": [
    "# 现在重新训练模型,并且使用已缩放的数据进行训练\n",
    "gd_lr = GDLinearRegression(n_iter=3000, eta=0.05, tol=0.00001)\n",
    "gd_lr.train(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在 eta 大幅提高到了0.05, 经过136次迭代后算法收敛,损失约 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "在测试集上的MSE: 0.4336\n在测试集上的MAE: 0.4965\n"
    }
   ],
   "source": [
    "# 最后使用训练好的模型对测试集中的实例进行预测,并评估性能\n",
    "y_pred = gd_lr.predict(X_test_std)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('在测试集上的MSE: {:.4f}'.format(mse))\n",
    "print('在测试集上的MAE: {:.4f}'.format(mae))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bitf9d8712fa2a44abfab611268866e8143",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}